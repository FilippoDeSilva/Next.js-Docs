name: üìö Next.js Docs Downloader

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * 0"  # Weekly update

jobs:
  download-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install playwright beautifulsoup4 requests tqdm
          playwright install --with-deps

      - name: Download Next.js Docs
        run: |
          python - <<'EOF'
          import os
          import json
          import time
          from playwright.sync_api import sync_playwright
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin
          from pathlib import Path
          import requests
          from tqdm import tqdm

          BASE_URL = "https://nextjs.org/docs"
          OUTPUT_DIR = "nextjs-docs"
          os.makedirs(OUTPUT_DIR, exist_ok=True)

          def get_all_links():
              """Extract all documentation links from the sidebar"""
              with sync_playwright() as p:
                  browser = p.chromium.launch(headless=True)
                  context = browser.new_context()
                  page = context.new_page()
                  
                  print("üåê Loading documentation page...")
                  page.goto(BASE_URL, wait_until="networkidle", timeout=60000)
                  
                  # Wait for the sidebar to load
                  page.wait_for_selector('nav[aria-label="Main"]', timeout=60000)
                  
                  # Extract all documentation links
                  links = page.eval_on_selector_all(
                      'nav[aria-label="Main"] a[href^="/docs/"]',
                      'els => els.map(e => e.href)'
                  )
                  
                  # Clean and deduplicate links
                  unique_links = set()
                  for link in links:
                      if link.startswith('/'):
                          link = f"https://nextjs.org{link}"
                      if link.startswith(BASE_URL):
                          unique_links.add(link)
                  
                  browser.close()
                  return sorted(unique_links)

          def clean_html(html, url):
              """Clean up the HTML content"""
              soup = BeautifulSoup(html, 'html.parser')
              
              # Remove unwanted elements
              selectors = [
                  'header', 'footer', 'nav', 'aside',
                  '[class*="sidebar"]', '[class*="header"]', '[class*="footer"]',
                  '[class*="toc"]', '[class*="feedback"]', '[class*="edit"]',
                  '[class*="pagination"]', '[class*="breadcrumb"]', '[class*="search"]',
                  'script', 'style', 'svg', 'iframe', 'noscript',
                  'button', 'form', 'input', 'select', 'textarea',
                  '[role="alert"]', '[role="banner"]', '[role="dialog"]',
                  '[role="navigation"]', '[role="search"]', '[role="tab"]'
              ]
              
              for selector in selectors:
                  for el in soup.select(selector):
                      el.decompose()
              
              # Update image sources to absolute URLs
              for img in soup.find_all('img', src=True):
                  if not img['src'].startswith(('http', 'data:')):
                      img['src'] = urljoin(url, img['src'])
              
              # Add basic styling
              style = soup.new_tag('style')
              style.string = '''
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                      line-height: 1.6;
                      max-width: 800px;
                      margin: 0 auto;
                      padding: 20px;
                      color: #333;
                  }
                  pre {
                      background: #f6f8fa;
                      padding: 16px;
                      border-radius: 6px;
                      overflow: auto;
                  }
                  code {
                      font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
                      background: #f6f8fa;
                      padding: 0.2em 0.4em;
                      border-radius: 3px;
                      font-size: 85%;
                  }
                  img {
                      max-width: 100%;
                      height: auto;
                  }
                  a {
                      color: #0070f3;
                      text-decoration: none;
                  }
                  a:hover {
                      text-decoration: underline;
                  }
              '''
              if soup.head:
                  soup.head.append(style)
              
              return str(soup)

          def save_page(url, index):
              """Save a single documentation page"""
              try:
                  response = requests.get(url, timeout=30)
                  if response.status_code == 200:
                      # Generate filename from URL
                      filename = url.replace(BASE_URL, '').strip('/').replace('/', '-')
                      if not filename:
                          filename = 'index'
                      filename = f"{index:03d}-{filename}.html"
                      filepath = os.path.join(OUTPUT_DIR, filename)
                      
                      # Clean and save the HTML
                      cleaned_html = clean_html(response.text, url)
                      with open(filepath, 'w', encoding='utf-8') as f:
                          f.write(cleaned_html)
                      
                      return True
                  return False
              except Exception as e:
                  print(f"Error saving {url}: {str(e)}")
                  return False

          def main():
              print("üöÄ Starting Next.js documentation download...")
              
              # Get all documentation links
              links = get_all_links()
              print(f"üîó Found {len(links)} documentation pages")
              
              # Download each page
              success_count = 0
              for i, url in enumerate(tqdm(links, desc="üì• Downloading pages")):
                  if save_page(url, i + 1):
                      success_count += 1
                  time.sleep(1)  # Be nice to the server
              
              print(f"‚úÖ Successfully downloaded {success_count}/{len(links)} pages")
              print(f"üìÅ Documentation saved to: {os.path.abspath(OUTPUT_DIR)}")

          if __name__ == "__main__":
              main()
          EOF

      - name: Create archive
        run: |
          tar -czvf nextjs-docs-$(date +%Y%m%d).tar.gz nextjs-docs/

      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: nextjs-docs
          path: nextjs-docs-*.tar.gz
          retention-days: 7

      - name: Commit changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add nextjs-docs/
          git commit -m "üìö Update Next.js documentation [skip ci]"
          git push
        continue-on-error: true
