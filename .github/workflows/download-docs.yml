name: üß≠ Next.js Docs PDF (Playwright Full-Fidelity)

on:
  workflow_dispatch:
    inputs:
      limit:
        description: 'Limit pages per group (0 = all, e.g., 10 for trial)'
        required: false
        default: '10'

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      LIMIT: ${{ github.event.inputs.limit }}

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 playwright PyPDF2
          playwright install --with-deps chromium

      - name: Write PDF generator script
        run: |
          cat << 'EOF' > generate_docs.py
          import asyncio
          from playwright.async_api import async_playwright
          import requests, os, time
          from bs4 import BeautifulSoup
          from PyPDF2 import PdfMerger

          BASE_URL = "https://nextjs.org/docs"
          HEADERS = {"User-Agent": "Mozilla/5.0"}
          LIMIT = int(os.getenv("LIMIT", "10"))  # 0 means no limit

          def log(msg):
              print(msg, flush=True)

          def get_links():
              log("üîç Fetching documentation links...")
              html = requests.get(BASE_URL, headers=HEADERS, timeout=20).text
              soup = BeautifulSoup(html, "html.parser")

              app_links, pages_links = [], []
              for a in soup.find_all("a", href=True):
                  href = a["href"]
                  if not href.startswith("/docs/"):
                      continue
                  full = f"https://nextjs.org{href}"
                  if "/docs/app/" in href and full not in app_links:
                      app_links.append(full)
                  elif "/docs/pages/" in href and full not in pages_links:
                      pages_links.append(full)

              if LIMIT > 0:
                  app_links = app_links[:LIMIT]
                  pages_links = pages_links[:LIMIT]

              log(f"üìÑ Found {len(app_links)} App and {len(pages_links)} Pages docs (limit={LIMIT})")
              return app_links, pages_links

          async def wait_for_images(page, timeout_sec=10):
              async def images_ready():
                  return await page.evaluate("""() => {
                      const imgs = Array.from(document.images);
                      return imgs.length === 0 || imgs.every(img => img.complete && img.naturalHeight !== 0);
                  }""")
              start = time.time()
              while time.time() - start < timeout_sec:
                  if await images_ready():
                      return True
                  await asyncio.sleep(0.5)
              return False

          async def wait_for_fonts(page, timeout_sec=10):
              try:
                  await asyncio.wait_for(page.evaluate("document.fonts.ready.then(() => true)"), timeout=timeout_sec)
                  return True
              except asyncio.TimeoutError:
                  return False

          async def trigger_lazy_loading(page, max_steps=20):
              # Smoothly scroll to bottom to trigger lazy-loaded content
              await page.evaluate("""async (steps) => {
                  const total = steps;
                  for (let i = 0; i < total; i++) {
                      window.scrollBy(0, Math.floor(window.innerHeight * 0.8));
                      await new Promise(r => setTimeout(r, 200));
                  }
                  window.scrollTo(0, document.body.scrollHeight);
              }""", max_steps)

          def sanitize_filename(text):
              return "".join(c for c in text if c.isalnum() or c in (" ", "_", "-", ".")).strip().replace(" ", "_")

          async def render_group(group, links):
              os.makedirs("pdfs", exist_ok=True)
              pdf_paths = []
              async with async_playwright() as p:
                  browser = await p.chromium.launch()
                  context = await browser.new_context(user_agent=HEADERS["User-Agent"])
                  for idx, url in enumerate(links, start=1):
                      log(f"\nüåê ({idx}/{len(links)}) Navigating to: {url}")
                      page = await context.new_page()
                      try:
                          start = time.time()
                          await page.goto(url, wait_until="load", timeout=25000)
                          await trigger_lazy_loading(page)

                          # Wait for images and fonts with bounded timeouts
                          imgs_ok = await wait_for_images(page, timeout_sec=12)
                          fonts_ok = await wait_for_fonts(page, timeout_sec=8)
                          title = await page.title()
                          log(f"üìù Title: {title} | üñºÔ∏è Images: {'ready' if imgs_ok else 'timeout'} | üî§ Fonts: {'ready' if fonts_ok else 'timeout'} | ‚è±Ô∏è {round(time.time() - start, 2)}s")

                          # Remove unwanted site chrome and expand main content
                          await page.add_style_tag(content="""
                              header, footer, nav, aside,
                              .sidebar, .docs-sidebar, [aria-label="Sidebar"],
                              .feedback, .docFeedback, .feedback-widget,
                              .docs-header, .docs-footer,
                              .navigation-container, .breadcrumbs,
                              .version-switcher, .theme-switcher,
                              .site-header, .site-footer, #__next > div > header, #__next > div > footer {
                                  display: none !important;
                                  visibility: hidden !important;
                                  height: 0 !important;
                              }
                              .main, .docs-content, .content, main[role="main"], #__next main {
                                  margin: 0 auto !important;
                                  width: 100% !important;
                                  max-width: 100% !important;
                              }
                              body {
                                  background: #ffffff !important;
                                  color: #111111 !important;
                              }
                              * {
                                  scroll-behavior: auto !important;
                              }
                          """)

                          safe_title = sanitize_filename(title) or f"{group}_{idx:02d}"
                          filename = f"pdfs/{group}_{idx:02d}_{safe_title}.pdf"
                          await page.pdf(
                              path=filename,
                              format="A4",
                              print_background=True,
                              margin={"top": "1cm", "bottom": "1cm", "left": "1cm", "right": "1cm"}
                          )
                          log(f"üì¶ Saved PDF: {filename}")
                          pdf_paths.append(filename)
                      except Exception as e:
                          log(f"‚ùå Failed to render {url}: {e}")
                      finally:
                          await page.close()
                  await browser.close()
              return pdf_paths

          def merge_pdfs(paths, output):
              log(f"\nüß© Merging {len(paths)} PDFs into: {output}")
              merger = PdfMerger()
              for path in paths:
                  log(f"‚ûï Adding: {path}")
                  merger.append(path)
              merger.write(output)
              merger.close()
              log(f"üéâ Final PDF ready: {output}")

          async def main():
              app_links, pages_links = get_links()
              app_pdfs = await render_group("AppRouter", app_links)
              pages_pdfs = await render_group("PagesRouter", pages_links)
              all_pdfs = app_pdfs + pages_pdfs
              if not all_pdfs:
                  log("‚ö†Ô∏è No PDFs generated ‚Äî exiting.")
                  return
              merge_pdfs(all_pdfs, "NextJS_Docs_Archive.pdf")

          asyncio.run(main())
          EOF

      - name: Run PDF generation
        run: python generate_docs.py

      - name: Upload PDF artifact
        uses: actions/upload-artifact@v4
        with:
          name: nextjs-docs-archive
          path: NextJS_Docs_Archive.pdf