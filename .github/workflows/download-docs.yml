name: ğŸ§­ Next.js Docs to PDF (Latest Only)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 6 * * *" # run daily at 6AM UTC

jobs:
  build:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: ğŸ§¹ Checkout repository
        uses: actions/checkout@v4

      - name: âš™ï¸ Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: ğŸ“¦ Install dependencies
        run: |
          pip install aiohttp beautifulsoup4 weasyprint

      - name: ğŸ“– Generate Next.js Docs PDF
        run: |
          python - <<'EOF'
          import asyncio, aiohttp, re
          from bs4 import BeautifulSoup
          from weasyprint import HTML, CSS

          BASE_URL = "https://nextjs.org/docs"
          HEADERS = {"User-Agent": "Mozilla/5.0"}

          async def get_links(session):
              print("ğŸ” Extracting links from sidebar...")
              async with session.get(BASE_URL, headers=HEADERS) as resp:
                  html = await resp.text()
                  soup = BeautifulSoup(html, "html.parser")
                  links = []
                  for a in soup.find_all("a", href=True):
                      href = a["href"]
                      if (
                          href.startswith("/docs/")
                          and not any(href.startswith(f"/docs/{v}/") for v in ["13", "14"])
                          and not href.startswith("/docs/api-reference") # skip low-value pages
                      ):
                          links.append(f"https://nextjs.org{href}")
                  return list(dict.fromkeys(links))

          async def fetch_page(session, url):
              try:
                  async with session.get(url, headers=HEADERS) as resp:
                      html = await resp.text()
                      soup = BeautifulSoup(html, "html.parser")

                      # Remove unwanted UI parts
                      for selector in [
                          "header", "footer", "nav",
                          ".feedback-module__j8fpJW__inlineWrapper",
                          ".feedback-module__j8fpJW__inlineWrapperClosed",
                          ".sticky.top-[121px].hidden.h-[calc(100vh-121px)].w-[284px]"
                      ]:
                          for tag in soup.select(selector):
                              tag.decompose()

                      main = soup.find("main") or soup
                      title = soup.title.string if soup.title else url

                      # Preserve formatting â€” headings, code, bold, lists
                      for tag in main.find_all(True):
                          if tag.name in ["code", "pre"]:
                              tag["style"] = "background:#f4f4f4; padding:4px 6px; border-radius:4px; font-family:monospace; display:block; white-space:pre-wrap;"
                          elif tag.name in ["h1","h2","h3"]:
                              tag["style"] = "color:#000;font-weight:600;margin-top:20px;"
                          elif tag.name in ["p"]:
                              tag["style"] = "margin:6px 0;line-height:1.5;font-size:14px;"
                          elif tag.name in ["ul","ol"]:
                              tag["style"] = "margin-left:18px;line-height:1.4;"
                      
                      body_html = f"<h1 style='font-size:22px;color:#0070f3'>{title}</h1>" + str(main)
                      return body_html
              except Exception as e:
                  print(f"âŒ Error fetching {url}: {e}")
                  return ""

          async def scrape_docs():
              async with aiohttp.ClientSession() as session:
                  links = await get_links(session)
                  print(f"ğŸ“„ Found {len(links)} valid pages.")
                  docs_html = ""
                  for i, link in enumerate(links, 1):
                      print(f"({i}/{len(links)}) Fetching {link}")
                      page_html = await fetch_page(session, link)
                      if page_html:
                          docs_html += page_html + "<hr>"
                  return docs_html

          async def main():
              html_content = await scrape_docs()
              print("ğŸ§¾ Rendering final PDF...")
              css = CSS(string="""
                  @page { margin: 1cm; }
                  body { font-family: Arial, sans-serif; color: #222; }
                  h1, h2, h3 { color: #111; }
                  code { background: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
                  pre { background: #f4f4f4; padding: 6px; border-radius: 4px; overflow-x: auto; }
              """)
              HTML(string=html_content, base_url=BASE_URL).write_pdf("NextJS_Latest_Docs.pdf", stylesheets=[css])
              print("âœ… PDF generation complete!")

          asyncio.run(main())
          EOF

      - name: ğŸ’¾ Commit & Push PDF
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          mv NextJS_Latest_Docs.pdf nextjs_docs_latest.pdf
          git add nextjs_docs_latest.pdf
          git commit -m "ğŸ“˜ Update Next.js Docs PDF (latest only)"
          git push

      - name: ğŸ“¤ Upload PDF as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: nextjs-docs-latest
          path: nextjs_docs_latest.pdf
