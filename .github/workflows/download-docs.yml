name: Next.js Docs PDF

on:
  workflow_dispatch:  # Manual trigger
  schedule:          # Nightly trigger at midnight UTC
    - cron: '0 0 * * *'

jobs:
  build-docs:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: 3.11

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pyppeteer weasyprint tqdm

      - name: Generate Docs PDF
        env:
          BASE_URL: "https://nextjs.org/docs"
          OUT_DIR: "docs_html"
          PDF_FILE: "Nextjs_Docs_Dark.pdf"
        run: |
          mkdir -p $OUT_DIR
          python - <<'PYTHON_CODE'
import os, requests, asyncio, time
from bs4 import BeautifulSoup
from pyppeteer import launch
from pathlib import Path

BASE_URL = os.environ["BASE_URL"]
OUT_DIR = Path(os.environ["OUT_DIR"])
PDF_FILE = os.environ["PDF_FILE"]

OUT_DIR.mkdir(exist_ok=True)

visited_urls = set()

def fetch_page(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"❌ Failed to fetch {url}: {e}")
        return None

# Get all docs links from main page
main_html = fetch_page(BASE_URL)
soup = BeautifulSoup(main_html, "html.parser")
links = [a["href"] for a in soup.select("a[href^='/docs']")]
links = list(set(BASE_URL + l for l in links))

print(f"Found {len(links)} pages to download...")

# Save each doc page
for link in links:
    if link in visited_urls:
        continue
    visited_urls.add(link)
    try:
        html_content = fetch_page(link)
        if not html_content:
            continue
        filename = OUT_DIR / (link.strip("/").split("/")[-1] or "index")
        filename.with_suffix(".html").write_text(html_content, encoding="utf-8")
        print(f"✅ Saved {link}")
        time.sleep(0.5)  # polite delay
    except Exception as e:
        print(f"❌ Error saving {link}: {e}")

# Render PDF with dark mode
async def render_pdf():
    browser = await launch(args=["--no-sandbox"])
    page = await browser.newPage()

    # Combine all HTML pages into one
    combined_html = ""
    for f in sorted(OUT_DIR.glob("*.html")):
        content = f.read_text(encoding="utf-8")
        combined_html += content + "<div style='page-break-after: always;'></div>"

    await page.setContent(combined_html, {"waitUntil": "networkidle2"})
    # Dark mode styling
    dark_css = """
        html, body { background: #0d1117 !important; color: #c9d1d9 !important; }
        h1,h2,h3,h4,h5,h6 { color: #58a6ff !important; }
        a { color: #58a6ff !important; }
        code, pre { background: #161b22 !important; color: #f0f6fc !important; }
    """
    await page.addStyleTag({"content": dark_css})

    await page.pdf({
        "path": PDF_FILE,
        "format": "A4",
        "printBackground": True
    })
    await browser.close()
    print(f"📄 PDF saved: {PDF_FILE}")

asyncio.get_event_loop().run_until_complete(render_pdf())
PYTHON_CODE